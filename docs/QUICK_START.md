# üöÄ Gu√≠a R√°pida de Uso - ExamGenerator con IA

## ‚úÖ Pruebas Realizadas con √âxito

Todas las funcionalidades han sido probadas y verificadas:

### ‚úÖ Prueba 1: Creaci√≥n de Documento
```bash
# Creado: documento_ia.docx (714 caracteres)
# Tema: Conceptos B√°sicos de Inteligencia Artificial
```

### ‚úÖ Prueba 2: Generaci√≥n con Ollama
```bash
python qg.py documento_ia.docx --motor ollama --modelo gemma3:4b --num_preguntas 3

Resultado:
üìÑ Extrayendo texto del DOCX: documento_ia.docx...
‚úÖ Texto extra√≠do exitosamente (714 caracteres)
‚úÖ Servidor Ollama ya est√° corriendo en http://localhost:11434

ü§ñ Enviando texto a Ollama local (gemma3:4b)...
üì° URL de Ollama: http://localhost:11434

‚úÖ ¬°Aqu√≠ tienes las preguntas generadas! ‚úÖ
------------------------------------------------------------
Preguntas generadas en espa√±ol sobre IA
------------------------------------------------------------

üìä Motor usado: OLLAMA | Modelo: gemma3:4b
```

### ‚úÖ Prueba 3: Auto-Inicio de Ollama
```bash
# Ollama detenido ‚Üí Script detecta ausencia ‚Üí Pregunta si iniciar ‚Üí Inicia autom√°ticamente
```

## üìñ Comandos de Ejemplo

### Uso B√°sico
```bash
# Con Gemini (requiere API key)
python qg.py documento.pdf --num_preguntas 10

# Con Ollama (local)
python qg.py documento.pdf --motor ollama --num_preguntas 10
```

### Con Modelos Espec√≠ficos
```bash
# Gemini Pro (m√°s preciso)
python qg.py apuntes.pdf --motor gemini --modelo gemini-1.5-pro --num_preguntas 15

# Ollama con Mistral
python qg.py tema.docx --motor ollama --modelo mistral --num_preguntas 8

# Ollama con Gemma 3
python qg.py presentacion.pptx --motor ollama --modelo gemma3:4b --num_preguntas 12
```

### Comparar Motores
```bash
# Mismo documento, diferentes motores
python qg.py contenido.pdf --motor gemini --num_preguntas 10
python qg.py contenido.pdf --motor ollama --modelo phi4 --num_preguntas 10
```

### Diferentes Idiomas
```bash
# En ingl√©s
python qg.py document.pdf --motor ollama --idioma english --num_preguntas 10

# En espa√±ol (default)
python qg.py documento.pdf --motor ollama --num_preguntas 10
```

## üéØ Casos de Uso Real

### Estudiante Preparando Examen
```bash
# 1. Tienes apuntes en PDF
python qg.py apuntes_tema3.pdf --motor ollama --modelo gemma3:4b --num_preguntas 20

# 2. Las preguntas se generan autom√°ticamente
# 3. Puedes estudiar con ellas
```

### Profesor Creando Material
```bash
# 1. Tienes presentaci√≥n del tema
python qg.py clase_quimica.pptx --motor ollama --modelo mistral --num_preguntas 15

# 2. Guardas las preguntas
python qg.py clase_quimica.pptx --motor ollama --num_preguntas 15 > preguntas_quimica.txt

# 3. Las usas con eg.py para generar ex√°menes
python eg.py preguntas_quimica.txt Parcial 3 15
```

### Empresa Capacitando Personal
```bash
# 1. Manual de procedimientos en DOCX
python qg.py manual_seguridad.docx --motor ollama --modelo llama2 --num_preguntas 25

# 2. Preguntas para evaluaci√≥n de conocimientos
```

## üîç Verificar Configuraci√≥n

```bash
# Ejecutar script de verificaci√≥n
python test_setup.py

# Ver modelos de Ollama disponibles
ollama list

# Ver ayuda de qg.py
python qg.py --help
```

## üí° Tips Pr√°cticos

### Optimizar Velocidad
- Usa modelos m√°s peque√±os: `gemma3:4b` es m√°s r√°pido que `phi4`
- Reduce el n√∫mero de preguntas para pruebas r√°pidas
- Usa Gemini si tienes buena conexi√≥n a internet

### Maximizar Calidad
- Usa `gemini-1.5-pro` para mejor precisi√≥n
- Usa `mistral` en Ollama para an√°lisis de texto
- Genera m√°s preguntas y selecciona las mejores

### Trabajar Offline
- Usa siempre `--motor ollama`
- Ten varios modelos descargados
- Descarga modelos con: `ollama pull nombre_modelo`

## üö® Soluci√≥n de Problemas

### Ollama no inicia autom√°ticamente
```bash
# Iniciarlo manualmente una vez
ollama serve

# Luego usar normalmente
python qg.py documento.pdf --motor ollama
```

### Modelo no encontrado
```bash
# Descargar el modelo
ollama pull llama2

# Verificar modelos instalados
ollama list

# Usar modelo existente
python qg.py doc.pdf --motor ollama --modelo phi4
```

### Error de encoding en Windows
```bash
# Configurar UTF-8
$env:PYTHONIOENCODING="utf-8"

# Ejecutar comando
python qg.py documento.pdf --motor ollama
```

## üìä Comparaci√≥n de Velocidad (Aproximada)

Basado en documento de ~1000 palabras, 10 preguntas:

| Motor | Modelo | Tiempo | Calidad | Internet |
|-------|--------|--------|---------|----------|
| Gemini | flash | ~5-10s | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Requiere |
| Gemini | pro | ~10-20s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Requiere |
| Ollama | gemma3:4b | ~30-60s | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå Local |
| Ollama | phi4 | ~60-120s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå Local |
| Ollama | mistral | ~40-80s | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå Local |

*Tiempos var√≠an seg√∫n hardware. GPU acelera Ollama significativamente.*

## üéì Flujo Completo de Trabajo

```bash
# 1. Verificar configuraci√≥n
python test_setup.py

# 2. Generar preguntas de tu material de estudio
python qg.py material.pdf --motor ollama --modelo gemma3:4b --num_preguntas 20

# 3. Copiar las preguntas a un archivo preguntas.txt (formato del proyecto)

# 4. Generar ex√°menes aleatorios
python eg.py preguntas.txt MiExamen 5 10 both plantilla.docx xlsx

# 5. Tienes 5 ex√°menes diferentes con 10 preguntas cada uno
```

## üåü Ejemplos Avanzados

### Pipeline Completo
```bash
# Generar preguntas, guardar y crear ex√°menes
python qg.py libro_cap1.pdf --motor ollama --num_preguntas 30 > temp_preguntas.txt
# Formatear manualmente temp_preguntas.txt al formato de preguntas.txt
python eg.py preguntas.txt Cap1 10 15 docx plantilla.docx
```

### M√∫ltiples Documentos
```bash
# Generar preguntas de varios temas
python qg.py tema1.pdf --motor ollama --num_preguntas 10 > tema1_q.txt
python qg.py tema2.pdf --motor ollama --num_preguntas 10 > tema2_q.txt
python qg.py tema3.pdf --motor ollama --num_preguntas 10 > tema3_q.txt

# Combinar manualmente en preguntas.txt
# Generar ex√°menes mixtos
python eg.py preguntas.txt Repaso 3 30
```

---

**√öltima actualizaci√≥n**: 11 Enero 2026
**Versi√≥n**: 10.20260111.3
**Estado**: ‚úÖ Totalmente funcional y probado
