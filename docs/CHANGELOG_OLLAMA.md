# âœ¨ Nueva Funcionalidad: Soporte para IA Local con Ollama

## ğŸ“ Resumen de Cambios

Se ha aÃ±adido soporte completo para **Ollama**, permitiendo generar preguntas usando IA local sin necesidad de conexiÃ³n a internet ni API keys de pago.

## ğŸ¯ Archivos Modificados

### 1. `qg.py` - Generador con IA
**Cambios principales:**
- âœ… Eliminados conflictos de merge (`<<<<<<< HEAD`)
- âœ… AÃ±adida funciÃ³n `generate_questions_with_gemini()` - Motor Google Gemini
- âœ… AÃ±adida funciÃ³n `generate_questions_with_ollama()` - Motor Ollama local
- âœ… Nuevos argumentos de lÃ­nea de comandos:
  - `--motor`: Elegir entre `gemini` u `ollama`
  - `--modelo`: Especificar modelo (llama2, mistral, gemini-1.5-pro, etc.)
  - `--ollama_url`: URL personalizada del servidor Ollama
- âœ… Imports condicionales para mejor manejo de dependencias
- âœ… Mensajes de error mejorados en espaÃ±ol

### 2. `requirements.txt` - Dependencias
**Cambios:**
- âœ… AÃ±adido `requests>=2.31.0` para comunicaciÃ³n con Ollama
- âœ… Eliminados conflictos de merge

### 3. `.github/copilot-instructions.md` - Instrucciones para IA
**Nuevas secciones:**
- âœ… DocumentaciÃ³n de arquitectura dual (Gemini + Ollama)
- âœ… ExplicaciÃ³n de modelo de selecciÃ³n
- âœ… Ejemplos de uso de ambos motores
- âœ… Patrones de manejo de errores especÃ­ficos de Ollama

### 4. `OLLAMA_SETUP.md` - GuÃ­a de ConfiguraciÃ³n (NUEVO)
**Contenido:**
- âœ… GuÃ­a completa de instalaciÃ³n de Ollama
- âœ… Instrucciones paso a paso por sistema operativo
- âœ… Ejemplos prÃ¡cticos de uso
- âœ… ComparaciÃ³n Gemini vs Ollama
- âœ… SoluciÃ³n de problemas comunes
- âœ… Mejores prÃ¡cticas

## ğŸš€ CÃ³mo Usar las Nuevas Funcionalidades

### OpciÃ³n 1: Google Gemini (Como antes)
```bash
# Configurar API key
$env:GOOGLE_API_KEY = "tu-api-key"

# Usar
python qg.py documento.pdf --num_preguntas 10
```

### OpciÃ³n 2: Ollama Local (NUEVO)
```bash
# 1. Instalar Ollama desde https://ollama.ai

# 2. Iniciar servidor
ollama serve

# 3. Descargar modelo
ollama pull llama2

# 4. Usar con ExamGenerator
python qg.py documento.pdf --motor ollama --num_preguntas 10
```

## ğŸ¨ Ejemplos Avanzados

### Comparar ambos motores
```bash
# Mismo documento con ambos motores
python qg.py tema.pdf --motor gemini --num_preguntas 10
python qg.py tema.pdf --motor ollama --modelo mistral --num_preguntas 10
```

### Usar modelo especÃ­fico
```bash
# Gemini Pro (mÃ¡s preciso)
python qg.py doc.pdf --motor gemini --modelo gemini-1.5-pro

# Mistral (excelente para texto)
python qg.py doc.pdf --motor ollama --modelo mistral
```

### Servidor Ollama remoto
```bash
python qg.py doc.pdf --motor ollama --ollama_url http://192.168.1.100:11434
```

## ğŸ“Š Ventajas de Ollama

âœ… **100% Privado** - Datos nunca salen de tu computadora
âœ… **Gratuito** - Sin lÃ­mites ni costos de API
âœ… **Sin Internet** - Funciona completamente offline
âœ… **MÃºltiples Modelos** - llama2, mistral, codellama, etc.
âœ… **Open Source** - Transparencia total

## âš ï¸ Consideraciones

### Ollama requiere:
- 8GB RAM mÃ­nimo (recomendado 16GB)
- 4-8GB de espacio en disco por modelo
- CPU moderna (GPU opcional pero acelera)

### Google Gemini requiere:
- ConexiÃ³n a internet
- API key de Google AI
- Posibles costos segÃºn uso

## ğŸ”§ InstalaciÃ³n de Dependencias

Si ya tienes el proyecto instalado, solo necesitas actualizar:

```bash
# Activar entorno virtual
.\.venv\Scripts\Activate.ps1  # Windows
source .venv/bin/activate      # Linux/macOS

# Instalar nueva dependencia
pip install requests>=2.31.0

# O reinstalar todo
pip install -r requirements.txt
```

## ğŸ“š DocumentaciÃ³n

- **GuÃ­a de Ollama**: Ver `OLLAMA_SETUP.md`
- **Instrucciones AI**: Ver `.github/copilot-instructions.md`
- **README principal**: Ver `README.md` (secciÃ³n qg.py)

## ğŸ› SoluciÃ³n de Problemas

### Error: "No se puede conectar a Ollama"
```bash
# Asegurarse de que Ollama estÃ© corriendo
ollama serve
```

### Error: "Modelo no encontrado"
```bash
# Descargar el modelo
ollama pull llama2
```

### Error: "Module 'requests' not found"
```bash
# Instalar requests
pip install requests
```

## ğŸ¯ PrÃ³ximos Pasos Recomendados

1. **Probar con Gemini** (si ya tienes API key):
   ```bash
   python qg.py preguntas.txt --motor gemini --num_preguntas 5
   ```

2. **Instalar y probar Ollama**:
   - Descargar desde https://ollama.ai
   - Seguir guÃ­a en `OLLAMA_SETUP.md`

3. **Comparar resultados** de ambos motores con el mismo documento

4. **Actualizar README.md** si quieres aÃ±adir mÃ¡s ejemplos

## ğŸ“ Notas de ImplementaciÃ³n

- **Compatibilidad**: CÃ³digo retrocompatible, funciona sin cambios si solo usas Gemini
- **Imports condicionales**: El cÃ³digo verifica dependencias antes de importar
- **Manejo de errores**: Mensajes claros en espaÃ±ol para ambos motores
- **Timeout**: 5 minutos para Ollama (configurable en cÃ³digo)
- **Default**: Gemini sigue siendo el motor por defecto para no romper scripts existentes

## ğŸ”„ MigraciÃ³n desde VersiÃ³n Anterior

Si usabas qg.py antes, **no necesitas cambiar nada**. SeguirÃ¡ funcionando igual:

```bash
# Esto sigue funcionando exactamente igual
python qg.py documento.pdf --num_preguntas 10
```

Para usar Ollama, solo aÃ±ade `--motor ollama`:

```bash
# VersiÃ³n con Ollama
python qg.py documento.pdf --motor ollama --num_preguntas 10
```

---

**VersiÃ³n**: 10.20260111 (11 Enero 2026)
**Autor**: ActualizaciÃ³n por TiiZss
**Licencia**: GPL v3
