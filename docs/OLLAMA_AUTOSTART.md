# ğŸš€ Auto-Inicio de Ollama - Nueva Funcionalidad

## ğŸ“ DescripciÃ³n

ExamGenerator ahora **inicia automÃ¡ticamente el servidor Ollama** si no estÃ¡ corriendo, haciendo mucho mÃ¡s fÃ¡cil el uso de IA local.

## âœ¨ CÃ³mo Funciona

### Antes (Manual)
```bash
# TenÃ­as que hacer esto primero:
ollama serve

# Y luego en otra terminal:
python qg.py documento.pdf --motor ollama
```

### Ahora (AutomÃ¡tico) âš¡
```bash
# Solo ejecuta esto:
python qg.py documento.pdf --motor ollama

# Si Ollama no estÃ¡ corriendo, el script pregunta:
# "Â¿Quieres que intente iniciar Ollama automÃ¡ticamente? (s/n):"
# 
# Responde 's' y:
# âœ… Inicia Ollama
# âœ… Espera a que estÃ© listo
# âœ… ContinÃºa automÃ¡ticamente
```

## ğŸ”§ Detalles TÃ©cnicos

### Funciones AÃ±adidas en qg.py

1. **`check_ollama_running(ollama_url)`**
   - Verifica si Ollama responde en la URL especificada
   - Retorna `True` si estÃ¡ corriendo, `False` si no

2. **`start_ollama_server()`**
   - Detecta el sistema operativo (Windows/Linux/macOS)
   - Inicia Ollama usando el mÃ©todo apropiado:
     - **Windows**: Intenta `net start Ollama` o `ollama serve` en nueva consola
     - **Linux/macOS**: Ejecuta `ollama serve` en segundo plano
   - Espera hasta 30 segundos para que el servidor inicie
   - Muestra progreso cada 5 segundos

3. **`ensure_ollama_running(ollama_url)`**
   - Verifica si Ollama estÃ¡ corriendo
   - Si no estÃ¡, pregunta al usuario si quiere iniciarlo
   - Llama a `start_ollama_server()` si el usuario acepta
   - Retorna `True` si Ollama estÃ¡ disponible, `False` si no

### IntegraciÃ³n

La funciÃ³n `generate_questions_with_ollama()` ahora llama a `ensure_ollama_running()` antes de procesar:

```python
def generate_questions_with_ollama(...):
    # ... cÃ³digo de validaciÃ³n ...
    
    # NUEVO: Verificar y asegurar que Ollama estÃ© corriendo
    if not ensure_ollama_running(ollama_url):
        return None
    
    # Continuar con la generaciÃ³n de preguntas
    # ...
```

## ğŸ“Š Flujo de EjecuciÃ³n

```
Usuario ejecuta: python qg.py doc.pdf --motor ollama
                          â†“
        Â¿EstÃ¡ Ollama corriendo?
                â†™              â†˜
              SÃ               NO
               â†“                â†“
    Continuar normal    Preguntar al usuario
                              â†“
                   Â¿Iniciar automÃ¡ticamente?
                       â†™              â†˜
                     SÃ               NO
                      â†“                â†“
              Iniciar Ollama    Mostrar error
                      â†“           y terminar
              Esperar 30s
                      â†“
            Â¿IniciÃ³ correctamente?
                â†™              â†˜
              SÃ               NO
               â†“                â†“
    Continuar normal    Mostrar error
                           y terminar
```

## ğŸ¯ Casos de Uso

### Caso 1: Primera Vez (Usuario Nuevo)
```bash
$ python qg.py apuntes.pdf --motor ollama

ğŸ“„ Extrayendo texto del PDF: apuntes.pdf...
âœ… Texto extraÃ­do exitosamente (2345 caracteres)
âš ï¸  Servidor Ollama no detectado en http://localhost:11434
Â¿Quieres que intente iniciar Ollama automÃ¡ticamente? (s/n): s
ğŸš€ Intentando iniciar el servidor Ollama...
â³ Esperando a que Ollama inicie...
   Esperando... (5s)
âœ… Servidor Ollama iniciado correctamente

ğŸ¤– Enviando texto a Ollama local (llama2)...
# ContinÃºa normalmente...
```

### Caso 2: Ollama Ya Corriendo
```bash
$ python qg.py apuntes.pdf --motor ollama

ğŸ“„ Extrayendo texto del PDF: apuntes.pdf...
âœ… Texto extraÃ­do exitosamente (2345 caracteres)
âœ… Servidor Ollama ya estÃ¡ corriendo en http://localhost:11434

ğŸ¤– Enviando texto a Ollama local (llama2)...
# ContinÃºa normalmente...
```

### Caso 3: Usuario Prefiere Manual
```bash
$ python qg.py apuntes.pdf --motor ollama

ğŸ“„ Extrayendo texto del PDF: apuntes.pdf...
âœ… Texto extraÃ­do exitosamente (2345 caracteres)
âš ï¸  Servidor Ollama no detectado en http://localhost:11434
Â¿Quieres que intente iniciar Ollama automÃ¡ticamente? (s/n): n
âŒ No se puede continuar sin Ollama
   Inicia Ollama manualmente con: ollama serve
```

## ğŸ” DetecciÃ³n por Sistema Operativo

### Windows
1. Intenta iniciar el servicio Windows: `net start Ollama`
2. Si falla, ejecuta `ollama serve` en nueva ventana de consola
3. Verifica cada segundo si el servidor responde

### Linux/macOS
1. Ejecuta `ollama serve` en segundo plano
2. Redirige stdout/stderr a /dev/null
3. Verifica cada segundo si el servidor responde

## âš ï¸ Manejo de Errores

### Ollama No Instalado
```
âŒ Error: Ollama no estÃ¡ instalado o no estÃ¡ en PATH
   Descarga Ollama desde: https://ollama.ai
```

### Timeout (30 segundos)
```
âš ï¸  Ollama no respondiÃ³ en 30 segundos
```

### Error Durante Inicio
```
âŒ Error al intentar iniciar Ollama: [detalles del error]
```

## ğŸ› ï¸ ConfiguraciÃ³n Avanzada

### Servidor Remoto
Si usas Ollama en otra mÃ¡quina, el auto-inicio no se ejecuta:

```bash
python qg.py doc.pdf --motor ollama --ollama_url http://192.168.1.100:11434
```

En este caso, el script solo verifica si el servidor remoto estÃ¡ disponible.

### Timeout Personalizable
Puedes modificar el timeout en el cÃ³digo (por defecto 30 segundos):

```python
# En start_ollama_server()
for i in range(30):  # <-- Cambiar este nÃºmero
    time.sleep(1)
    if check_ollama_running(...):
        return True
```

## ğŸ“¦ Dependencias Nuevas

```python
import subprocess  # Para ejecutar comandos del sistema
import time        # Para esperar a que Ollama inicie
import sys         # Para control de flujo
import platform    # Para detectar el SO
```

Todas estas son librerÃ­as estÃ¡ndar de Python, **no requieren instalaciÃ³n adicional**.

## ğŸ§ª Testing

### Probar Auto-Inicio
```bash
# 1. Asegurarte de que Ollama NO estÃ© corriendo
# Windows:
net stop Ollama

# Linux/macOS:
pkill ollama

# 2. Ejecutar qg.py
python qg.py documento.pdf --motor ollama

# 3. Responder 's' cuando pregunte
# 4. Verificar que inicie correctamente
```

### Probar DetecciÃ³n
```bash
# 1. Iniciar Ollama manualmente
ollama serve

# 2. En otra terminal, ejecutar
python qg.py documento.pdf --motor ollama

# 3. Debe detectar que ya estÃ¡ corriendo
# Salida esperada: "âœ… Servidor Ollama ya estÃ¡ corriendo..."
```

## ğŸ“š Archivos Modificados

1. **qg.py**
   - AÃ±adidas 3 nuevas funciones
   - Modificada `generate_questions_with_ollama()`
   - AÃ±adidos imports: `subprocess`, `time`, `sys`, `platform`

2. **test_setup.py**
   - Actualizado mensaje cuando Ollama no estÃ¡ corriendo
   - AÃ±adido hint sobre auto-inicio

3. **OLLAMA_SETUP.md**
   - Nueva secciÃ³n "âš¡ Inicio AutomÃ¡tico"
   - Actualizada secciÃ³n de soluciÃ³n de problemas

4. **.github/copilot-instructions.md**
   - Documentada funcionalidad de auto-inicio
   - Actualizada arquitectura de Ollama

## ğŸ’¡ Mejores PrÃ¡cticas

1. **Primera vez**: Dejar que el script inicie Ollama automÃ¡ticamente
2. **Uso frecuente**: Considerar dejar Ollama corriendo como servicio
3. **Desarrollo**: Iniciar Ollama manualmente para ver logs
4. **ProducciÃ³n**: Configurar Ollama como servicio del sistema

## ğŸ“ Ventajas

âœ… **Experiencia de usuario mejorada** - Un comando menos que recordar
âœ… **Menos fricciÃ³n** - Especialmente Ãºtil para usuarios nuevos
âœ… **Multiplataforma** - Funciona en Windows, Linux y macOS
âœ… **Seguro** - Pregunta antes de iniciar, no lo hace sin permiso
âœ… **Informativo** - Muestra progreso y mensajes claros
âœ… **Robusto** - Maneja errores y timeouts correctamente

## ğŸ”„ Compatibilidad

- âœ… Retrocompatible: Si prefieres iniciar Ollama manualmente, sigue funcionando
- âœ… No invasivo: Solo actÃºa si Ollama NO estÃ¡ corriendo
- âœ… Respeta configuraciÃ³n: Usa la URL personalizada si se proporciona
- âœ… No requiere dependencias extra: Solo librerÃ­as estÃ¡ndar de Python

---

**VersiÃ³n**: 10.20260111.2
**Fecha**: 11 Enero 2026
**CaracterÃ­stica**: Auto-inicio de Ollama
