# ü§ñ Gu√≠a de Configuraci√≥n de Ollama para ExamGenerator

## üìã Descripci√≥n

ExamGenerator ahora soporta **dos motores de IA** para generar preguntas autom√°ticamente:

1. **Google Gemini** (Nube) - Requiere API key y conexi√≥n a internet
2. **Ollama** (Local) - IA completamente local, privada y gratuita

## üöÄ Configuraci√≥n R√°pida de Ollama

### Paso 1: Instalar Ollama

#### Windows
```powershell
# Descargar desde https://ollama.ai y ejecutar el instalador
# O usar winget:
winget install Ollama.Ollama
```

#### Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### macOS
```bash
# Descargar desde https://ollama.ai
# O usar brew:
brew install ollama
```

### Paso 2: Iniciar el Servidor

```bash
ollama serve
```

El servidor se iniciar√° en `http://localhost:11434`

### Paso 3: Descargar Modelos

```bash
# Modelo recomendado para empezar (r√°pido y eficiente)
ollama pull llama2

# Otros modelos excelentes:
ollama pull mistral      # Muy bueno para an√°lisis de texto
ollama pull codellama    # Especializado en contenido t√©cnico
ollama pull llama2:13b   # M√°s preciso pero m√°s pesado
```

### Paso 4: Verificar Instalaci√≥n

```bash
# Listar modelos instalados
ollama list

# Probar un modelo
ollama run llama2 "Hola, ¬øc√≥mo est√°s?"
```

## üìñ Uso con ExamGenerator

### ‚ö° Inicio Autom√°tico (NUEVO)

**ExamGenerator ahora puede iniciar Ollama autom√°ticamente** si no est√° corriendo. Simplemente ejecuta:

```bash
python qg.py documento.pdf --motor ollama
```

Si Ollama no est√° corriendo, el script te preguntar√°:
```
‚ö†Ô∏è  Servidor Ollama no detectado en http://localhost:11434
¬øQuieres que intente iniciar Ollama autom√°ticamente? (s/n):
```

Responde `s` y el script:
1. ‚úÖ Iniciar√° Ollama autom√°ticamente
2. ‚úÖ Esperar√° a que est√© listo
3. ‚úÖ Continuar√° con la generaci√≥n de preguntas

### Sintaxis Completa

```bash
python qg.py <archivo> [opciones]

Opciones:
  --num_preguntas N     N√∫mero de preguntas a generar (default: 10)
  --idioma IDIOMA       Idioma (default: espa√±ol)
  --motor MOTOR         Motor: gemini u ollama (default: gemini)
  --modelo MODELO       Modelo espec√≠fico
  --ollama_url URL      URL del servidor Ollama (default: http://localhost:11434)
```

### Ejemplos Pr√°cticos

#### Ejemplo 1: Uso B√°sico
```bash
# Generar 10 preguntas de un PDF usando Ollama
python qg.py documento.pdf --motor ollama
```

#### Ejemplo 2: Especificar Modelo
```bash
# Usar Mistral para mejor calidad
python qg.py apuntes.pdf --motor ollama --modelo mistral --num_preguntas 15
```

#### Ejemplo 3: Procesar PowerPoint
```bash
# Generar preguntas de una presentaci√≥n
python qg.py presentacion.pptx --motor ollama --modelo llama2 --num_preguntas 20
```

#### Ejemplo 4: Servidor Ollama Remoto
```bash
# Conectar a un servidor Ollama en otra m√°quina
python qg.py documento.pdf --motor ollama --ollama_url http://192.168.1.100:11434
```

#### Ejemplo 5: Comparar Motores
```bash
# Con Gemini (nube)
python qg.py tema.pdf --motor gemini --num_preguntas 10

# Con Ollama (local)
python qg.py tema.pdf --motor ollama --num_preguntas 10
```

## üéØ Modelos Recomendados por Caso de Uso

| Caso de Uso | Modelo Recomendado | Comando |
|-------------|-------------------|---------|
| General / R√°pido | `llama2` | `ollama pull llama2` |
| Mejor Calidad | `mistral` | `ollama pull mistral` |
| Contenido T√©cnico | `codellama` | `ollama pull codellama` |
| M√°s Precisi√≥n | `llama2:13b` | `ollama pull llama2:13b` |
| Multiling√ºe | `mixtral` | `ollama pull mixtral` |

## ‚öôÔ∏è Requisitos del Sistema

### Para Ollama
- **RAM**: M√≠nimo 8GB (recomendado 16GB)
- **Espacio en Disco**: 
  - llama2: ~4GB
  - mistral: ~4GB
  - llama2:13b: ~8GB
- **CPU/GPU**: Funciona con CPU, GPU acelera la generaci√≥n

### Para ExamGenerator
```bash
pip install requests>=2.31.0
```

## üîß Soluci√≥n de Problemas

### Error: "No se puede conectar a Ollama"

**Problema**: `‚ùå Error: No se puede conectar a Ollama en http://localhost:11434`

**Soluci√≥n Autom√°tica** (NUEVO):
```bash
# El script te preguntar√° si quieres iniciar Ollama autom√°ticamente
# Solo responde 's' cuando aparezca el mensaje
python qg.py documento.pdf --motor ollama
```

**Soluci√≥n Manual**:
```bash
# Verificar que Ollama est√© ejecut√°ndose
ollama serve

# En otra terminal, verificar que responde
curl http://localhost:11434/api/tags
```

### Error: "Modelo no encontrado"

**Problema**: El modelo especificado no existe

**Soluci√≥n**:
```bash
# Listar modelos disponibles
ollama list

# Descargar el modelo necesario
ollama pull llama2
```

### Timeout / Muy Lento

**Problema**: La generaci√≥n toma demasiado tiempo

**Soluciones**:
1. Usar un modelo m√°s peque√±o: `llama2` en lugar de `llama2:13b`
2. Reducir el tama√±o del documento fuente
3. Usar menos preguntas: `--num_preguntas 5`
4. Considerar usar GPU si est√° disponible

## üìä Comparaci√≥n: Gemini vs Ollama

| Caracter√≠stica | Google Gemini | Ollama |
|----------------|---------------|--------|
| **Ubicaci√≥n** | Nube | Local |
| **Privacidad** | Datos en Google | 100% Privado |
| **Internet** | Requerido | No necesario |
| **Costo** | API Key (puede tener costo) | Gratuito |
| **Velocidad** | Muy r√°pida | Depende del hardware |
| **Calidad** | Excelente | Muy buena |
| **Instalaci√≥n** | Solo API key | Requiere software |
| **Recursos** | Ninguno local | RAM y CPU/GPU |

## üéì Mejores Pr√°cticas

1. **Para Documentos Peque√±os** (<10 p√°ginas):
   ```bash
   python qg.py doc.pdf --motor ollama --modelo llama2
   ```

2. **Para Documentos Grandes** (>50 p√°ginas):
   ```bash
   # Usar Gemini (m√°s r√°pido) o dividir el documento
   python qg.py doc.pdf --motor gemini --num_preguntas 20
   ```

3. **Para M√°xima Privacidad**:
   ```bash
   # Siempre usar Ollama
   python qg.py confidencial.pdf --motor ollama
   ```

4. **Para Mejor Calidad**:
   ```bash
   # Usar Mistral o Gemini Pro
   python qg.py examen.pdf --motor ollama --modelo mistral
   # O
   python qg.py examen.pdf --motor gemini --modelo gemini-1.5-pro
   ```

## üîó Enlaces √ötiles

- **Ollama**: https://ollama.ai
- **Modelos Disponibles**: https://ollama.ai/library
- **Documentaci√≥n Ollama**: https://github.com/ollama/ollama
- **Google Gemini**: https://ai.google.dev/

## üí° Tips Adicionales

1. **Mantener Ollama actualizado**:
   ```bash
   # Windows: Reinstalar desde ollama.ai
   # Linux:
   curl -fsSL https://ollama.ai/install.sh | sh
   ```

2. **Liberar espacio eliminando modelos**:
   ```bash
   ollama rm nombre_modelo
   ```

3. **Ver uso de recursos**:
   ```bash
   ollama ps  # Ver modelos en ejecuci√≥n
   ```

4. **Ejecutar Ollama en segundo plano**:
   ```bash
   # Linux/macOS:
   ollama serve &
   
   # Windows: El servicio se instala autom√°ticamente
   ```

## ‚ö†Ô∏è Warnings Comunes (Normales)

### "Phi SWA is currently disabled"

**¬øQu√© significa?** Phi SWA (Sliding Window Attention) es una optimizaci√≥n avanzada que no est√° habilitada en llama.cpp.

**¬øAfecta la calidad?** ‚ùå **NO** - El modelo funciona perfectamente. Este es solo un aviso t√©cnico.

**Acci√≥n:** Ignorar. Si quieres evitarlo, usa `gemma2:2b` o `llama3.2:1b` en lugar de `phi3:mini`.

### "n_ctx_seq (4096) < n_ctx_train (131072)"

**¬øQu√© significa?** El modelo fue entrenado con contexto de 131K tokens, pero Ollama usa 4K por defecto.

**¬øAfecta la calidad?** ‚ùå **NO para documentos normales** - 4096 tokens ‚âà 3000 palabras, suficiente para generar 10-20 preguntas.

**Solo afecta si:** Procesas documentos **muy largos** (100+ p√°ginas).

**Aumentar contexto (si necesario):**
```bash
# Temporal (una ejecuci√≥n)
ollama run phi3:mini --ctx-size 8192

# Permanente: Editar docker-compose.yml
ollama:
  environment:
    OLLAMA_NUM_CTX: 8192  # Requiere m√°s RAM (~8GB)
```

---

**¬øNecesitas ayuda?** Abre un issue en https://github.com/TiiZss/ExamGenerator
